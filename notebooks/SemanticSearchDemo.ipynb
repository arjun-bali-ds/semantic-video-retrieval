{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa772d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/Users/abali/Documents/github projects/semantic-video-retrieval\")\n",
    "from clip_embedding import embed_frame,embed_text\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b901d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from extract_chunks_updated import extract_chunks\n",
    "# ‚úÖ Set paths\n",
    "video_folder = \"/Users/abali/Documents/github projects/semantic-video-retrieval/data/videos\"\n",
    "frames_output_folder = \"/Users/abali/Documents/github projects/semantic-video-retrieval/data/frames\"\n",
    "# index_path = \"/Users/abali/github projects/semantic-video-retrieval/embeddings/faiss_index\"\n",
    "# metadata_path = \"/Users/abali/github projects/semantic-video-retrieval/embeddings/metadata\"\n",
    "\n",
    "# ‚úÖ Extract chunks from videos\n",
    "all_metadata = []\n",
    "for fname in os.listdir(video_folder):\n",
    "    if fname.endswith(\".mp4\"):\n",
    "        video_path = os.path.join(video_folder, fname)\n",
    "        print(f\"üì¶ Processing {fname}...\")\n",
    "        chunks = extract_chunks(video_path, frames_output_folder, chunk_duration=15, fps=1)\n",
    "        all_metadata.extend(chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60dcbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_chunk_embeddings(chunk_folder_root):\n",
    "    \"\"\"\n",
    "    Given a directory of chunks (each containing frames), compute the average embedding\n",
    "    for each chunk and return a list of {chunk_id, embedding}.\n",
    "    \"\"\"\n",
    "    chunk_embeddings = []\n",
    "\n",
    "    for chunk_name in sorted(os.listdir(chunk_folder_root)):\n",
    "        chunk_path = os.path.join(chunk_folder_root, chunk_name)\n",
    "        if not os.path.isdir(chunk_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"üîç Processing {chunk_name}...\")\n",
    "        frame_embeddings = []\n",
    "\n",
    "        for fname in sorted(os.listdir(chunk_path)):\n",
    "            if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                image_path = os.path.join(chunk_path, fname)\n",
    "                emb = embed_frame(image_path)\n",
    "                if emb is not None and not np.isnan(emb).any():\n",
    "                    frame_embeddings.append(emb)\n",
    "        \n",
    "        if frame_embeddings:\n",
    "            mean_emb = np.mean(frame_embeddings, axis=0).astype(\"float32\")\n",
    "            chunk_embeddings.append({\n",
    "                \"chunk_id\": chunk_name,\n",
    "                \"embedding\": mean_emb\n",
    "            })\n",
    "            print(f\"‚úÖ Embedded: {chunk_name} ({len(frame_embeddings)} frames)\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No valid frames found for {chunk_name}\")\n",
    "\n",
    "    return chunk_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a99a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_embeddings=get_chunk_embeddings('/Users/abali/Documents/github projects/semantic-video-retrieval/data/chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37738793",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd102c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def store_embeddings_to_faiss(embedding_data, index_path, metadata_path):\n",
    "    if not embedding_data:\n",
    "        print(\"‚ö†Ô∏è No embeddings to store.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(os.path.dirname(index_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(metadata_path), exist_ok=True)\n",
    "\n",
    "    embeddings = [item[\"embedding\"] for item in embedding_data]\n",
    "    metadata = [{\"chunk_id\": item[\"chunk_id\"]} for item in embedding_data]\n",
    "\n",
    "    arr = np.vstack(embeddings).astype(\"float32\")\n",
    "    print(f\"üìê FAISS index shape: {arr.shape}\")\n",
    "\n",
    "    index = faiss.IndexFlatL2(arr.shape[1])\n",
    "    index.add(arr)\n",
    "\n",
    "    faiss.write_index(index, index_path)\n",
    "    with open(metadata_path, \"wb\") as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "    print(f\"‚úÖ Stored {len(arr)} vectors in FAISS and metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9da728",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_embeddings_to_faiss(chunk_embeddings, '/Users/abali/github projects/semantic-video-retrieval/embeddings/faiss_index/video_chunks.index', '/Users/abali/github projects/semantic-video-retrieval/embeddings/metadata/chunk_metadata.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98818854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_top_chunks(query, index_path, metadata_path, k=5):\n",
    "    # Load index\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load metadata\n",
    "    with open(metadata_path, \"rb\") as f:\n",
    "        metadata = pickle.load(f)\n",
    "\n",
    "    # Embed query\n",
    "    query_embedding = embed_text(query).astype(\"float32\").reshape(1, -1)\n",
    "\n",
    "    # Search\n",
    "    D, I = index.search(query_embedding, k)\n",
    "\n",
    "    # Collect results\n",
    "    results = []\n",
    "    for i in I[0]:\n",
    "        if i < len(metadata):\n",
    "            results.append(metadata[i])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_top_chunks(\n",
    "    query=\"moving cars\",\n",
    "    index_path=\"/Users/abali/github projects/semantic-video-retrieval/embeddings/faiss_index/video_chunks.index\",\n",
    "    metadata_path=\"/Users/abali/github projects/semantic-video-retrieval/embeddings/metadata/chunk_metadata.pkl\",k=1)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"üé¨ Chunk: {r['chunk_id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Loop through the search results and play the video\n",
    "for r in results:\n",
    "    # Extract the base video file name (e.g., sample1.mp4 from sample1_chunk0)\n",
    "    video_name = r['chunk_id'].split('_')[0] + '.mp4'\n",
    "    \n",
    "    # Construct the full video path\n",
    "    video_path = os.path.join(\"/Users/abali/Documents/github projects/semantic-video-retrieval/data/videos\", video_name)\n",
    "\n",
    "    print(f\"üé¨ Chunk: {r['chunk_id']}\")\n",
    "\n",
    "    # Ensure the video exists\n",
    "    if os.path.exists(video_path):\n",
    "        print(f\"‚ñ∂Ô∏è Opening video {video_name}...\")\n",
    "\n",
    "        # Use ffplay to open the entire video\n",
    "        subprocess.Popen([\"open\", video_path])  # macOS-specific command to open video with default player\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Video not found: {video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f2c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/Users/abali/Documents/github projects/semantic-video-retrieval/embeddings/metadata/enriched_chunk_metadata.pkl\", \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "print(metadata[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37addac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample enhancement (you can adapt this based on your real folder structure)\n",
    "from pathlib import Path\n",
    "\n",
    "updated_metadata = []\n",
    "for entry in metadata:\n",
    "    chunk_id = entry['chunk_id']\n",
    "    video_id = chunk_id.split('_')[0]\n",
    "    chunk_folder = Path(f\"/Users/abali/Documents/github projects/semantic-video-retrieval/data/chunks/{chunk_id}\")\n",
    "    frame_files = sorted(chunk_folder.glob(\"frame_*.jpg\"))\n",
    "\n",
    "    enriched = {\n",
    "        \"chunk_id\": chunk_id,\n",
    "        \"video_id\": video_id,\n",
    "        \"frame_paths\": [str(p) for p in frame_files],\n",
    "        \"num_frames\": len(frame_files),\n",
    "        \"start_frame\": int(frame_files[0].stem.split('_')[-1]) if frame_files else None,\n",
    "        \"end_frame\": int(frame_files[-1].stem.split('_')[-1]) if frame_files else None,\n",
    "    }\n",
    "    updated_metadata.append(enriched)\n",
    "\n",
    "# Save this enriched metadata for graph construction\n",
    "import pickle\n",
    "with open(\"/Users/abali/Documents/github projects/semantic-video-retrieval/embeddings/metadata/enriched_chunk_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(updated_metadata, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "\n",
    "local_path = \"/Users/abali/Documents/github projects/semantic-video-retrieval/local_models/vit-gpt2-image-captioning\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(local_path)\n",
    "processor = ViTImageProcessor.from_pretrained(local_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e239586",
   "metadata": {},
   "source": [
    "## Adding Image Description to Enrich Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c78089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "local_path = \"/Users/abali/Documents/github projects/semantic-video-retrieval/local_models/blip-image-captioning-base\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(local_path)\n",
    "model = BlipForConditionalGeneration.from_pretrained(local_path).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20574538",
   "metadata": {},
   "source": [
    "## Adding Description,Entities/ Objects to metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f26fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer,\n",
    "    BlipProcessor, BlipForConditionalGeneration\n",
    ")\n",
    "import spacy\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# === CONFIG ===\n",
    "USE_BLIP = True  # üîÅ Toggle this to False to use ViT-GPT2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Load Captioning Model ===\n",
    "if USE_BLIP:\n",
    "    blip_path = \"/Users/abali/Documents/github projects/semantic-video-retrieval/local_models/blip-image-captioning-base\"\n",
    "    processor = BlipProcessor.from_pretrained(blip_path)\n",
    "    caption_model = BlipForConditionalGeneration.from_pretrained(blip_path).to(device)\n",
    "    tokenizer = processor.tokenizer  # BLIP uses same tokenizer object\n",
    "else:\n",
    "    vit_path = \"/Users/abali/Documents/github projects/semantic-video-retrieval/local_models/vit-gpt2-image-captioning\"\n",
    "    caption_model = VisionEncoderDecoderModel.from_pretrained(vit_path).to(device)\n",
    "    processor = ViTImageProcessor.from_pretrained(vit_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(vit_path)\n",
    "\n",
    "# === Load NLP and Object Detection Models ===\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "yolo_model = YOLO(\"yolov8n.pt\")  # Swap with yolov8m.pt if needed\n",
    "\n",
    "# === Frame Processors ===\n",
    "def generate_caption(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = caption_model.generate(pixel_values, max_length=20, num_beams=4)\n",
    "\n",
    "        caption = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Caption error on {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_entities(texts):\n",
    "    entities = set()\n",
    "    for txt in texts:\n",
    "        doc = nlp(txt)\n",
    "        entities.update([ent.text.lower() for ent in doc.ents])\n",
    "        entities.update([chunk.text.lower() for chunk in doc.noun_chunks])\n",
    "    return list(entities)\n",
    "\n",
    "def detect_objects(image_path):\n",
    "    try:\n",
    "        results = yolo_model(image_path)\n",
    "        objects = set()\n",
    "        for r in results:\n",
    "            for box in r.boxes:\n",
    "                cls_id = int(box.cls[0])\n",
    "                label = r.names[cls_id]\n",
    "                objects.add(label.lower())\n",
    "        return list(objects)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Detection error on {image_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# === Load Chunk Metadata ===\n",
    "with open(\"/Users/abali/Documents/github projects/semantic-video-retrieval/embeddings/metadata/enriched_chunk_metadata.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)\n",
    "\n",
    "# === Enrichment Loop ===\n",
    "for chunk in chunks:\n",
    "    frame_paths = chunk.get(\"frame_paths\", [])[:2]\n",
    "    captions = []\n",
    "    all_objects = set()\n",
    "\n",
    "    for frame_path in frame_paths:\n",
    "        if not os.path.exists(frame_path):\n",
    "            continue\n",
    "\n",
    "        caption = generate_caption(frame_path)\n",
    "        if caption:\n",
    "            captions.append(caption)\n",
    "\n",
    "        detected = detect_objects(frame_path)\n",
    "        all_objects.update(detected)\n",
    "\n",
    "    chunk[\"captions\"] = captions\n",
    "    chunk[\"caption_entities\"] = extract_entities(captions)\n",
    "    chunk[\"objects\"] = list(all_objects)\n",
    "    chunk[\"all_entities\"] = list(set(chunk[\"caption_entities\"]) | set(chunk[\"objects\"]))\n",
    "\n",
    "# === Save Output ===\n",
    "with open(\"/Users/abali/Documents/github projects/semantic-video-retrieval/embeddings/metadata/enriched_with_all.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n",
    "\n",
    "print(\"‚úÖ Full enrichment complete (captions + entities + objects)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efbe2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks[:2]:\n",
    "    print(f\"\\nüß© {chunk['chunk_id']}\")\n",
    "    print(f\"üñºÔ∏è  Captions: {chunk['captions']}\")\n",
    "    print(f\"üîç Objects: {chunk['objects']}\")\n",
    "    print(f\"üß† Caption Entities: {chunk['caption_entities']}\")\n",
    "    print(f\"üì¶ All Entities (merged): {chunk['all_entities']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b01ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/abali/Documents/github projects/semantic-video-retrieval/embeddings/metadata/enriched_with_all.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e701489",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc3dfe",
   "metadata": {},
   "source": [
    "## Graph Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a23a6",
   "metadata": {},
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    " ##### Load enriched chunks #####\n",
    "with open(\"/Users/abali/Documents/github projects/semantic-video-retrieval/embeddings/metadata/enriched_with_all.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)\n",
    "\n",
    "##### Initialize graph and model #####\n",
    "G = nx.Graph()\n",
    "caption_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "##### Add nodes to graph #####\n",
    "for chunk in chunks:\n",
    "    G.add_node(chunk[\"chunk_id\"], **chunk)  # All metadata goes in node\n",
    "\n",
    "##### Helper to compute caption embedding ===\n",
    "def embed_captions(captions):\n",
    "    if not captions:\n",
    "        return np.zeros((384,))\n",
    "    caption_text = \" \".join(captions)\n",
    "    embedding = caption_model.encode(caption_text, normalize_embeddings=True)\n",
    "    return embedding\n",
    "\n",
    "##### Compute pairwise edges ===\n",
    "alpha, beta = 0.6, 0.4\n",
    "threshold = 0.3  # only add edges with weight > threshold\n",
    "\n",
    "embeddings = {c[\"chunk_id\"]: embed_captions(c[\"captions\"]) for c in chunks}\n",
    "entities = {c[\"chunk_id\"]: set(c[\"all_entities\"]) for c in chunks}\n",
    "\n",
    "chunk_ids = [c[\"chunk_id\"] for c in chunks]\n",
    "\n",
    "for i in range(len(chunk_ids)):\n",
    "    for j in range(i + 1, len(chunk_ids)):\n",
    "        id1, id2 = chunk_ids[i], chunk_ids[j]\n",
    "        \n",
    "        emb_sim = cosine_similarity([embeddings[id1]], [embeddings[id2]])[0][0]\n",
    "        \n",
    "        ent1, ent2 = entities[id1], entities[id2]\n",
    "        if ent1 or ent2:\n",
    "            jaccard_sim = len(ent1 & ent2) / len(ent1 | ent2)\n",
    "        else:\n",
    "            jaccard_sim = 0.0\n",
    "        \n",
    "        final_weight = alpha * emb_sim + beta * jaccard_sim\n",
    "\n",
    "        if final_weight > threshold:\n",
    "            G.add_edge(id1, id2, weight=final_weight)\n",
    "\n",
    "print(f\"‚úÖ Graph built: {len(G.nodes)} nodes, {len(G.edges)} edges\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
